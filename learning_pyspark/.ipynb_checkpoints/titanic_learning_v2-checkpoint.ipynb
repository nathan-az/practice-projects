{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, VectorIndexer\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, struct, lit, coalesce, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom jars for xgboost pyspark functionality.\n",
    "sparkxgb is an open source wrapper for sparkxgb in Python. It is outdated. I have manually removed references to deprecated pyspark classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the volume of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [(col, df.where(df[col].isNull()).count()) for col in df.columns]\n",
    "total_rows = df.count()\n",
    "print(f\"Total samples: {total_rows}\")\n",
    "for col, num_missing in missing:\n",
    "    if num_missing > 0:\n",
    "        print(f\"Column: {col}, num missing: {num_missing} (% missing: {num_missing/total_rows*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 3/4 of Cabin data is missing (687/891). There are a total 148 unique values, so 204 of the non-missing samples are unique. There may be useful information here still (e.g. cabin area. i.e. treating A23 and A12 as equivalent). But for now we will omit the Cabin as a feature rather than imputing or giving XGB free reign. \n",
    "\n",
    "Embarked has a small amount missing. We can set these to 0 for the sake of one-hot encoding, but will have minimal effect on data. For age, a recursive tree model will be used to predict the remaining values (alternatively, simple mean could be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "# Sex, Cabin, and Embarked must be indexed and potentially encoded\n",
    "initial_features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]\n",
    "target = \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_distinct = [(col, df.select(col).distinct().count()) for col in df.columns]\n",
    "# for col, num in num_distinct:\n",
    "#     print(f\"Column: {col}, unique value count: {num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep columns as features if we have an a priori reason to believe they would act as strong predictors. These include:\n",
    "Pclass, Age, Sex, SibSp, Parch, Fare, Cabin, Embarked\n",
    "\n",
    "Name, PassengerId, and Ticket are omitted as well due to being largely unique values without much discernible useful information (distinct calc above commented because it is very slow)\n",
    "\n",
    "Replace missing values with 0. Since we are doing so with categorical variables (for which 0 is not a value in the set), in essense, we are creating a new category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = OneHotEncoder(inputCols=[\"s1\"], outputCols=[\"catVec\"])\n",
    "features_for_encoding = [\"Sex\", \"Cabin\", \"Embarked\"]\n",
    "df2 = df\n",
    "for col in features_for_encoding:\n",
    "    df2 = df2.withColumn(col, when(df2[col].isNull(), 0).otherwise(df2[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the transformation pipeline then keep the transformer for use later on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in features_for_encoding]\n",
    "inputs = [indexer.getOutputCol() for indexer in indexers]\n",
    "encoder = OneHotEncoder(inputCols=inputs, outputCols=[col+\"_enc\" for col in features_for_encoding])\n",
    "stages = indexers + [encoder]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we transform the features to one-hot, vectorise them, then train and predict the basic decision tree classifier. The decision tree classifier really only has to be \"good enough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = pipeline.fit(df2)\n",
    "df3 = feature_transformer.transform(df2)\n",
    "feature_cols = ['Pclass',\n",
    "                 'Age',\n",
    "                 'SibSp',\n",
    "                 'Parch',\n",
    "                 'Fare',\n",
    "                 'Sex_enc',\n",
    "                 'Cabin_enc',\n",
    "                 'Embarked_enc']\n",
    "age_predictors = feature_cols.copy()\n",
    "age_predictors.remove(\"Age\")\n",
    "vectoriser = VectorAssembler().setInputCols(age_predictors).setOutputCol(\"Age_Predictors\")\n",
    "age_vectoriser = Pipeline(stages=[vectoriser])\n",
    "df3 = age_vectoriser.fit(df3).transform(df3)\n",
    "dt = DecisionTreeRegressor(featuresCol=\"Age_Predictors\", labelCol=\"Age\", maxDepth=5)\n",
    "df3_filtered = df3.filter(df3[\"Age\"].isNotNull())\n",
    "dt_model = dt.fit(df3_filtered)\n",
    "df3 = dt_model.transform(df3)\n",
    "\n",
    "\n",
    "df3 = df3.withColumn(\"age_predAge\", struct(df3[\"Age\"], df3[\"prediction\"]))\\\n",
    "        .withColumn(\"meanAge\", lit(29.7))\n",
    "df3 = df3.withColumn(\"age_meanAge\", struct(df3[\"Age\"], df3[\"meanAge\"]))\n",
    "\n",
    "train_acc_df = df3.filter(df3.Age.isNotNull()).select(\"age_predAge\", \"age_meanAge\")\n",
    "age_pred_rdd = train_acc_df.select(\"age_predAge\").rdd.flatMap(lambda x:x)\n",
    "age_mean_rdd = train_acc_df.select(\"age_meanAge\").rdd.flatMap(lambda x:x)\n",
    "\n",
    "age_pred_metrics = RegressionMetrics(age_pred_rdd)\n",
    "age_mean_metrics = RegressionMetrics(age_mean_rdd)\n",
    "\n",
    "print(f\"MSE using mean: {age_mean_metrics.rootMeanSquaredError:.2f}\")\n",
    "print(f\"MSE using prediction: {age_pred_metrics.rootMeanSquaredError:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have removed null values from categorical columns and made predictions for missing numerical values. All of the above has become a bit gross, largely due to creating duplicate DataFrames (for ease of debugging). Time to clean things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df3.select(age_predictors)\n",
    "age = df3.select(coalesce(df3.Age, df3.prediction))\n",
    "age = age.withColumnRenamed(\"coalesce(Age, prediction)\", \"Age\")\n",
    "df_final = df_final.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "age = age.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "df_final = df_final.join(age, on=[\"row_index\"]).drop(\"row_index\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = df_final.columns\n",
    "vectoriser = VectorAssembler().setInputCols(feature_cols).setOutputCol(\"features\")\n",
    "age_vectoriser = Pipeline(stages=[vectoriser])\n",
    "df_final = age_vectoriser.fit(df_final).transform(df_final).select(\"features\")\n",
    "df_final = df_final.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "df = df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id()))).select([\"row_index\", \"Survived\"])\n",
    "df_final = df_final.join(df, on=[\"row_index\"]).drop(\"row_index\")\n",
    "df_final.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
